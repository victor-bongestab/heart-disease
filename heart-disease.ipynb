{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE SUPPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import pickle\n",
    "\n",
    "# Data viz\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML packages\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import recall_score, f1_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uci_heart_data():\n",
    "    '''\n",
    "        Downloads dataset from the UCI Repository: Heart Disease.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: heart disease dataset.\n",
    "    '''\n",
    "    heart_disease = fetch_ucirepo(name='Heart Disease')\n",
    "\n",
    "    df_features = heart_disease.data.features\n",
    "    df_target = heart_disease.data.targets\n",
    "    df_full = pd.concat([df_features, df_target], axis=1)\n",
    "    \n",
    "    return df_full\n",
    "\n",
    "\n",
    "def recall_f1_cv(x_cv, y_cv, model_name, model, kfold=5, verbose=False):\n",
    "    '''\n",
    "    Cross-Validation for classifier models.\n",
    "\n",
    "    This CV helper function is made for non-ordinal classifier models,\n",
    "    thinking about the redution of False Negatives in our results.\n",
    "    Scores: Recall and F1.\n",
    "    '''\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    # We are going to use an index reset to apply our kfold limits.\n",
    "    df_aux = pd.concat([x_cv, y_cv], axis=1)\n",
    "    df_aux.reset_index(inplace=True, drop=True)\n",
    "    df_cv = df_aux.reset_index()\n",
    "\n",
    "    index_max = df_cv['index'].max()\n",
    "\n",
    "    for k in range( 1, (kfold + 1) ):\n",
    "        # This divides the data in kfold parts.\n",
    "        if verbose:\n",
    "            # For verbose=True, shows the KFold input.\n",
    "            print( '\\nKFold Number: {} / {}'.format(k, kfold) )\n",
    "        \n",
    "        # Parameter to include last index in the split.\n",
    "        z = 0\n",
    "        if k == kfold:\n",
    "            z = 1\n",
    "            \n",
    "        # Splitting train-validation.\n",
    "        df_train = df_cv[\n",
    "            (df_cv['index'] < (index_max * (k-1) / kfold)) | \n",
    "            (df_cv['index'] >= (index_max * (k+z) / kfold))]\n",
    "        if verbose:\n",
    "            print(df_train.shape)\n",
    "        \n",
    "        df_val = df_cv[\n",
    "            (df_cv['index'] >= (index_max * (k-1) / kfold)) & \n",
    "            (df_cv['index'] < (index_max * (k+z) / kfold))]\n",
    "        if verbose:\n",
    "            print(df_val.shape)\n",
    "\n",
    "        # Training dataset.\n",
    "        x_train = df_train.drop(['index', 'num'], axis=1)\n",
    "        y_train = df_train['num']\n",
    "\n",
    "        # Validation dataset.\n",
    "        x_val = df_val.drop(['index', 'num'], axis=1)\n",
    "        y_val = df_val['num']\n",
    "\n",
    "        # Model training.\n",
    "        m = model.fit( x_train, y_train )\n",
    "\n",
    "        # Prediction.\n",
    "        y_pred = m.predict(x_val)\n",
    "\n",
    "        # Storing Recall and F1 scores.\n",
    "        recall_list.append(recall_score(y_val, y_pred))\n",
    "        f1_list.append(f1_score(y_val, y_pred))\n",
    "\n",
    "    # Performance results\n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                        'Recall Score': np.round(np.mean(recall_list), 2).astype( str ),\n",
    "                        'F1 Score': np.round(np.mean(f1_list), 2).astype( str )},\n",
    "                        index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 BUSINESS UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP for Data Science\n",
    "\n",
    "This is the methodology I use in my projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading crisp img\n",
    "img = Image.open('img/crisp.png')\n",
    "\n",
    "# Show\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we begin tackling the problem by understanding it.\n",
    "\n",
    "Objective of this study:\n",
    "    - Supporting the development of a heart disease prevention system!\n",
    "\n",
    "My job as a Data Scientist is to create a predictive model to identify patients more likely to develop heart diseases.\n",
    "\n",
    "\n",
    "Dataset available in the URL from UC Irvine (UCI) ML Repository: https://archive.ics.uci.edu/ml/datasets/heart+disease.\n",
    "It's made of 4 databases: Cleveland, Hungary, Switzerland, and the VA Long Beach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I don't have support from a medical team, my decisions are solely made out of my dataset limited interpretation.\n",
    "\n",
    "Below, we can see a table with the variables information summary, taken directly from the UCI ML Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data_features img\n",
    "img = Image.open('img/dataset_variables.png')\n",
    "\n",
    "# Show\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Target feature also have this extra info:\n",
    "\n",
    "num: diagnosis of heart disease (angiographic disease status)\n",
    "        -- Value 0: < 50% diameter narrowing\n",
    "        -- Value 1: > 50% diameter narrowing\n",
    "        (in any major vessel: attributes 59 through 68 are vessels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have the Target feature, it's possible to use supervised models to predict whether or not new patients are prone to heart diseases.\n",
    "\n",
    "The next steps are Data Extraction and Data Cleaning. We can better understand the data he have by diving into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading timeline img\n",
    "img = Image.open('img/timeline.png')\n",
    "\n",
    "# Show\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the dataset from UCI.\n",
    "df1 = uci_heart_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we begin to understand how the dataset was built in more depth.\n",
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just taking a look at the dataframe.\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get rid of duplications, if they exist.\n",
    "df2.drop_duplicates()\n",
    "\n",
    "# Checking dataframe size.\n",
    "print( 'Rows: {}'.format( df2.shape[0] ) )\n",
    "print( 'Cols: {}'.format( df2.shape[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying information about variables types and missing values.\n",
    "type_value_analysis = pd.concat(\n",
    "    [df2.dtypes, df2.isna().sum()], \n",
    "    axis=1, \n",
    "    keys=['Data Types', 'Missing Values'])\n",
    "print(type_value_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to understand the missing values.\n",
    "df2[(df2['ca'].isna()) | (df2['thal'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't have any other information that could help determine the missing values \n",
    "# for 'ca' (flouroscopy result) or 'thal' (whatever this is). \n",
    "# \n",
    "# The decision here is to drop them. It's just 2.3% of our dataset, we'll survive.\n",
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to take a look in the variables with data type float64, as the \n",
    "# int64 alreary have their values determined by the nature of its type.\n",
    "# \n",
    "# These are 'oldpeak', 'ca' and 'thal'. Let's check their values. \n",
    "# \n",
    "# P.S.: UCI Repository says that 'oldpeak' and 'ca' are integer and 'thal' \n",
    "# is categorical.\n",
    "unique_values_dict = {\n",
    "    'oldpeak': list(df2['oldpeak'].unique()),\n",
    "    'ca': list(df2['ca'].unique()),\n",
    "    'thal': list(df2['thal'].unique())\n",
    "}\n",
    "\n",
    "for i in unique_values_dict:\n",
    "    print('{}: {}'.format(i, unique_values_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, 'ca' and 'thal' can be stored as integer. But, contrary to \n",
    "# the UCI recommendation, 'oldpeak' is indeed float.\n",
    "variables_to_integer = ['ca', 'thal']\n",
    "\n",
    "for i in variables_to_integer:\n",
    "    df2[i] = df2[i].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Identifying Categorical and Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display unique values for each variable. Then we can decide on\n",
    "# the nature of the data attributes.\n",
    "attributes = list(df2.columns)\n",
    "attributes_dict = {}\n",
    "\n",
    "for att in attributes:\n",
    "    attributes_dict[att] = list(df2[att].unique())\n",
    "\n",
    "for i, j in attributes_dict.items():\n",
    "    print('{}: {}'.format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values for 'num' are not as expected from the data description from UCI.\n",
    "# As we should have only 0 or 1, for False and True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete list of attributes:\n",
    "attributes = list(df2.columns)\n",
    "\n",
    "# For the numerical attributes, we can identify:\n",
    "num_attributes = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "# And for the categorical ones, we have:\n",
    "cat_attributes = [att for att in attributes if att not in num_attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Adjusting Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we saw, 'num' values are not as expected. We can follow \n",
    "# with 0 as 0, but 1, 2, 3, 4 as 1. \n",
    "# If I'd take a guess, it may identify from which of the four\n",
    "# databases these positives come from.\n",
    "df2.loc[df2['num'] > 0, 'num'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Identifying Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histogram to identify weird patterns.\n",
    "df2[num_attributes].hist(bins=20, figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histogram to identify weird patterns.\n",
    "df2[cat_attributes].hist(bins=20, figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything looks fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's adventure time.\n",
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hypothesis Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICAL ATTRIBUTES\n",
    "# 1. age - Risk patients tend to be older;\n",
    "\n",
    "# 2. trestbps - Risk patients are more common to have higher resting\n",
    "# blood pressure;\n",
    "\n",
    "# 3. chol - It's more common to find high serum cholesterol patients\n",
    "# among the risk group;\n",
    "\n",
    "# 4. thalach - maximum heart rate achieved higher for risk patients,\n",
    "# athletes tend to have lower heart rate overall;\n",
    "\n",
    "# 5. oldpeak - ST segment instability are common in patients with heart\n",
    "# disease.\n",
    "\n",
    "\n",
    "# CATEGORICAL ATTRIBUTES\n",
    "# 6. sex - Women (0) take more care of themselves and see doctors more \n",
    "# often, meaning lower risk of developing diseases in general;\n",
    "\n",
    "# 7. cp - chest pain (1, 2, 3) may indicate problems around the chest, \n",
    "# anginal (1, 2) often indicate a heart disease (value 4 should be more \n",
    "# common in healthy patients);\n",
    "\n",
    "# 8. fbs - high sugar (1) is usually related to heart diseases;\n",
    "\n",
    "# 9. restecg - abnormal (1) and compromised (2) ECG results should be\n",
    "# more often on risk patients;\n",
    "\n",
    "# 10. exang - angina by exercising (1) should indicate risk;\n",
    "\n",
    "# 11. slope - ST slope should be flat (2), most important cause of\n",
    "# ST segment abnormality is myocardial ischaemia or infarction \n",
    "# (source: litfl.com/st-segment-ecg-library/ accessed in 2024-sep-07);\n",
    "\n",
    "# 12. ca - I don't know what result we should get, let's assume\n",
    "# that lower vessels mean higher risk;\n",
    "\n",
    "# 13. thal - normal (3) means lower risk, defects (6, 7) should be \n",
    "# more common in risk patients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Descriptive Analysis for each Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='lightblue'> Univariate Analysis </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Tendency\n",
    "ct1 = pd.DataFrame( df3[num_attributes].apply( np.mean ) ).T\n",
    "ct2 = pd.DataFrame( df3[num_attributes].apply( np.median ) ).T\n",
    "\n",
    "# Dispersion - std, min, max, range, skew, kurtosis\n",
    "d1 = pd.DataFrame( df3[num_attributes].apply( np.std ) ).T\n",
    "d2 = pd.DataFrame( df3[num_attributes].apply( min ) ).T\n",
    "d3 = pd.DataFrame( df3[num_attributes].apply( max ) ).T\n",
    "d4 = pd.DataFrame( df3[num_attributes].apply( lambda x: x.max() - x.min() ) ).T\n",
    "d5 = pd.DataFrame( df3[num_attributes].apply( lambda x: x.skew() ) ).T\n",
    "d6 = pd.DataFrame( df3[num_attributes].apply( lambda x: x.kurtosis() ) ).T\n",
    "\n",
    "summary = pd.concat( [ d2, d3, d4, ct1, ct2, d1, d5, d6 ] ).T.reset_index()\n",
    "summary.columns = [ 'attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis' ]\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, we have distributions close to Normal, with attention\n",
    "# to 'chol' with high kurtosis, indicating a high peak around the mean value.\n",
    "# And 'oldpeak' with high positive skewness, showing its tendency to lower\n",
    "# values, and high kurtosis, also highlighting a great peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[num_attributes].hist( bins=80 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see an accumulation of 'oldpeak' in point zero and\n",
    "# this can be investigated further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='lightblue'> Bivariate Analysis </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'oldpeak' analysis just to get a hang of it.\n",
    "# Figure size.\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot for healthy patients.\n",
    "sns.kdeplot(df3.loc[df3['num'] == 0, 'oldpeak'], label='Healthy', color='dodgerblue', shade=True)\n",
    "\n",
    "# Plot for risk patients.\n",
    "sns.kdeplot(df3.loc[df3['num'] == 1, 'oldpeak'], label='Risk', color='darkred', shade=True)\n",
    "\n",
    "# Show legend and graph.\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that 'oldpeak' serves as a good indicator for heart health.\n",
    "# From that, we can generalize and see how the other variables behave too.\n",
    "for att in num_attributes:\n",
    "    # Figure settings.\n",
    "    plt.figure(figsize=(8, 3))\n",
    "\n",
    "    # Plots for healthy and risk patients\n",
    "    sns.kdeplot(df3.loc[df3['num'] == 0, att], label='Healthy', bw_adjust=0.5, color='dodgerblue', shade=True)\n",
    "    sns.kdeplot(df3.loc[df3['num'] == 1, att], label='Risk', bw_adjust=0.5, color='darkred', shade=True)\n",
    "\n",
    "    # Show plots.\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICAL ATTRIBUTES\n",
    "# 1. age - Risk patients tend to be older;\n",
    "#   * TRUE *\n",
    "\n",
    "# 2. trestbps - Risk patients are more common to have higher resting\n",
    "# blood pressure;\n",
    "#   * TRUE * But the difference between groups is small.\n",
    "\n",
    "# 3. chol - It's more common to find high serum cholesterol patients\n",
    "# among the risk group;\n",
    "#   * TRUE *\n",
    "\n",
    "# 4. thalach - maximum heart rate achieved higher for risk patients,\n",
    "# athletes tend to have lower heart rate overall;\n",
    "#   * FALSE * Looks like healthy people has some extra power.\n",
    "\n",
    "# 5. oldpeak - ST segment instability are common in patients with heart\n",
    "# disease.\n",
    "#   * TRUE * Very significant difference between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='lightpink'> Uni and Bivariate Analysis </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for att in cat_attributes:\n",
    "    # Target variable plot.\n",
    "    if att == 'num':\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(6, 3))\n",
    "        sns.countplot(x=att, data=df3)\n",
    "\n",
    "        # Show plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Go to next att.\n",
    "        continue\n",
    "\n",
    "    # Categorical Features plot.\n",
    "    # Figures settings.\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "    # Count of the attribute.\n",
    "    sns.countplot(x=att, data=df3, ax=axes[0])\n",
    "\n",
    "    # Comparison to the target variable.\n",
    "    sns.kdeplot(df3.loc[df3['num'] == 0, att], label='Healthy', color='dodgerblue', shade=True, bw_adjust=0.1, ax=axes[1])\n",
    "    sns.kdeplot(df3.loc[df3['num'] == 1, att], label='Risk', color='darkred', shade=True, bw_adjust=0.1, ax=axes[1])\n",
    "\n",
    "    # Show plots\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORICAL ATTRIBUTES\n",
    "# 6. sex - Women (0) take more care of themselves and see doctors more \n",
    "# often, meaning lower risk of developing diseases in general;\n",
    "#   * TRUE *\n",
    "\n",
    "# 7. cp - chest pain (1, 2, 3) may indicate problems around the chest, \n",
    "# anginal (1, 2) often indicate a heart disease (value 4 should be more \n",
    "# common in healthy patients);\n",
    "#   * FALSE * eXCUSE ME, WTf? Just found out that ischemia can be \n",
    "# * \"silent\"... Scary.\n",
    "\n",
    "# 8. fbs - high sugar (1) is usually related to heart diseases;\n",
    "#   * FALSE * Little to no difference between groups.\n",
    "\n",
    "# 9. restecg - abnormal (1) and compromised (2) ECG results should be\n",
    "# more often on risk patients;\n",
    "#   * TRUE *\n",
    "\n",
    "# 10. exang - angina by exercising (1) should indicate risk;\n",
    "#   * TRUE *\n",
    "\n",
    "# 11. slope - ST slope should be flat (2), most important cause of\n",
    "# ST segment abnormality is myocardial ischaemia or infarction \n",
    "# (source: litfl.com/st-segment-ecg-library/ accessed in 2024-sep-05);\n",
    "#   * FALSE * Apparently, the expected slope while exercising is \n",
    "# * different from rest slope. Up slope is the healthy one.\n",
    "\n",
    "# 12. ca - I don't know what result we should get, let's assume\n",
    "# that less color vessels mean higher risk;\n",
    "#   * FALSE * 0 is healthy, the others indicate risk. Further\n",
    "# * research let me understand that are multiple types of color-\n",
    "# * coding, depending on the purpose of the Angiography. We should\n",
    "# * get better info with the medical team about the procedure.\n",
    "\n",
    "# 13. thal - normal (3) means lower risk, defects (6, 7) should be \n",
    "# more common in risk patients.\n",
    "#   * TRUE *\n",
    "\n",
    "\n",
    "# Also, we have a lot of risk patients, almost half of our sample (> 50% diameter narrowing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can try to find other correlations between variables\n",
    "# and see how much each one impact the target variable compared\n",
    "# to one another.\n",
    "#\n",
    "# Seaborn pairplot in histograms can give us a good 2-D overview\n",
    "# of our numerical attributes related to target.\n",
    "pplot_attributes = num_attributes.copy()\n",
    "pplot_attributes.append('num')\n",
    "sns.pairplot(df3[pplot_attributes], hue = 'num', diag_kind='hist', plot_kws={'alpha':0.5});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an 2-dimensional hitplot, being the target variable indicated\n",
    "# by color. We are looking for diagonal shifts between the two\n",
    "# color spreads.\n",
    "#\n",
    "# Diagonal spreads mean greater correlation betwen two features,\n",
    "# and diagonal shifts mean they are both greatly affecting the target.\n",
    "#\n",
    "# Most diagonal-ish spreads:\n",
    "# age x thalach, age x trestbps, trestbps x thalach, \n",
    "# thalach x oldpeak.\n",
    "#\n",
    "# Most evident diagonal shifts:\n",
    "# age x thalach, age x oldpeak, thalach x oldpeak\n",
    "#\n",
    "# So we should give double attention to 'age', 'thalach' and 'oldpeak'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining a heatmap, we could confirm the assumptions above\n",
    "# and evaluate how the categorical attributes play along.\n",
    "#\n",
    "# Figures layout.\n",
    "fig,ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "# Getting the correlation matrix.\n",
    "corr = df3.corr()\n",
    "corr = corr.round(4)\n",
    "\n",
    "# Taking the upper triangle of the correlation matrix.\n",
    "matrix = np.triu(corr)\n",
    "\n",
    "# Using the upper triangle matrix as mask to only show what's left.\n",
    "sns.heatmap(corr, center=0, cmap='RdBu', annot=True, mask=matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can confirm that 'thalach' and 'oldpeak' have\n",
    "# the greatest influence over 'num', followed by 'age',\n",
    "# when watching only num_attributes.\n",
    "#\n",
    "# For cat_attributes, we can highlight 'cp', 'ca', 'exang'\n",
    "#  and 'thal', followed by 'sex' and 'slope'.\n",
    "#\n",
    "# Another interesting result is 'fbs' showing little to\n",
    "# no correlation to every other variable, except for\n",
    "# 'age', 'trestbps' and 'ca'. It may affect our target\n",
    "# indirectly but we should try taking it off our future\n",
    "# models to investigate its impact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further investigating the case of asymptomatic chest \n",
    "# pain, I found good information about how it's sex related\n",
    "# even though it looks to have no correlation according\n",
    "# to the heatmap above. \n",
    "# Now looking at how it differs from men to women.\n",
    "#\n",
    "# Plotting relative distribution of 'cp' for men and women,\n",
    "# healthy and in risk.\n",
    "# Figures settings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "def plot_kde_normalized(ax, data, label, color):\n",
    "    # Compute KDE\n",
    "    kde = gaussian_kde(data, bw_method=0.03)\n",
    "    x = np.linspace(1, 4, 1000)\n",
    "    y = kde(x)\n",
    "    \n",
    "    # Normalize to 100%\n",
    "    y = y / y.sum()\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(x, y, label=label, color=color)\n",
    "    ax.fill_between(x, y, color=color, alpha=0.2)\n",
    "\n",
    "# Women\n",
    "plot_kde_normalized(axes[0], df3.loc[(df3['num'] == 0) & (df3['sex'] == 0), 'cp'], label='Healthy', color='dodgerblue')\n",
    "plot_kde_normalized(axes[0], df3.loc[(df3['num'] == 1) & (df3['sex'] == 0), 'cp'], label='Risk', color='darkred')\n",
    "\n",
    "# Men\n",
    "plot_kde_normalized(axes[1], df3.loc[(df3['num'] == 0) & (df3['sex'] == 1), 'cp'], label='Healthy', color='dodgerblue')\n",
    "plot_kde_normalized(axes[1], df3.loc[(df3['num'] == 1) & (df3['sex'] == 1), 'cp'], label='Risk', color='darkred')\n",
    "\n",
    "\n",
    "# Adjusting layout, titles and legends\n",
    "axes[0].set_title('Women')\n",
    "axes[0].set_ylim(0, 0.11)\n",
    "axes[1].set_title('Men')\n",
    "axes[1].set_ylim(0, 0.11)\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical Angina: Great indicator for men in risk, it's not a symptom in women;\n",
    "# Atypical Angina: Great indicator for women in risk, less frequent in men;\n",
    "# Non-Anginal Chest Pain: Great indicator for men, not for women;\n",
    "# Asymptommatic: Way more common for women.\n",
    "#\n",
    "# With further research, I could find that the diagnosis for men and women are \n",
    "# quite different. Being chest pain, commonly spread as a must have sympton, \n",
    "# being greatly relevant for diagnosing men, but not women.\n",
    "#\n",
    "# According to the Hospital Einstein blog (https://vidasaudavel.einstein.br/infarto\n",
    "# -em-mulheres-conheca-mitos-e-verdades-sobre-ataque-cardiaco/, accessed in 2024-sep\n",
    "# -06):\n",
    "# \"[...] women describe chest pain as pressure or a tight feeling, and not as an \n",
    "# accute pain. Other frequent symptoms in female sex patients are:\n",
    "# Burning skin; pain in neck, shoulders, face and jaw; difficulty breathing; \n",
    "# fatigue; palpitations.\"\n",
    "#\n",
    "# From the National Library of Medicine (https://www.ncbi.nlm.nih.gov/medgen/149267#:\n",
    "# ~:text=Definition,from%20NCI%5D, accessed in 2024-sep-06), they define Atypical\n",
    "# Angina as:\n",
    "# \"Angina pectoris which does not have associated classical symptoms of chest pain. \n",
    "# Symptoms may include weakness, nausea, or sweating.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That show that we can not rely solely on a heatmap\n",
    "# to determine feature correlation. Specially because \n",
    "# the value 4 of 'cp' has huge impact on the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best correlation found until now was between\n",
    "# symptomns and sex, and I have a plan for it when\n",
    "# it's time to encode the cat_attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.plot(kind='box', subplots=True, layout=(2, 7), figsize=(20,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can observe that 'trestbps', 'chol' and 'odpeak' show\n",
    "# outliers. We can make a for loop to get rid of those cases.\n",
    "df3_filtered = df3.copy()\n",
    "\n",
    "for att in num_attributes:\n",
    "    # Getting Q1 and Q3.\n",
    "    q1 = np.percentile(df3_filtered[att], 25)\n",
    "    q3 = np.percentile(df3_filtered[att], 75)\n",
    "\n",
    "    # Then the Interquartile Range (IQR).\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # And determine Inferior and Superior Limits.\n",
    "    # Everything out of these boundaries are gonna be treated\n",
    "    # as outliers.\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    df3_filtered = df3_filtered[(df3_filtered[att] >= lower_bound) & \n",
    "                                (df3_filtered[att] <= upper_bound)]\n",
    "    \n",
    "print(f'Original df3 shape: {df3.shape}')\n",
    "print(f'Filtered df3 shape: {df3_filtered.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With outliers removed in our df3_filtered, let's\n",
    "# copy it to our df3.\n",
    "df3 = df3_filtered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 DATA MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to prepare our data for the ML algorithms.\n",
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I'm planning to use tree based models, I was not\n",
    "# going to scale our numerical attributes. But they always \n",
    "# work better when doing it for some reason.\n",
    "# Not much better, but it helps.\n",
    "#\n",
    "# MinMaxScaler for old_peak, because this one does not have a good\n",
    "# bell shape. And StandardScaler for the rest of the num attributes.\n",
    "cols_minmax = ['oldpeak']\n",
    "cols_std = num_attributes = [att for att in num_attributes if att != 'oldpeak']\n",
    "\n",
    "df4[cols_minmax] = MinMaxScaler().fit_transform(df4[cols_minmax])\n",
    "df4[cols_std] = StandardScaler().fit_transform(df4[cols_std])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for att in cat_attributes:\n",
    "    print('{}, values: {}'.format(att, df3[att].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex: fine (labeled);\n",
    "# * cp: It's already label encoded, but I'll reduce the amount of labels;\n",
    "# fbs: fine (labeled);\n",
    "# restecg: fine (ordinal);\n",
    "# exang: fine (labeled);\n",
    "# slope: fine (ordinal);\n",
    "# ca: fine (ordinal);\n",
    "# * thal: encoding reordination;\n",
    "# num: target, fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The'cp' values 1 and 3 are almost irrelevant for women.\n",
    "# On the other hand, they are good indicators of heart \n",
    "# disease for men.\n",
    "#\n",
    "# Value 2 is way more influent on women then on men.\n",
    "#\n",
    "# As a solution, we can separate male from female related \n",
    "# symptoms, as they are really dependent on sex. This will\n",
    "# help the ML algorithms work less.\n",
    "#\n",
    "# New Chest Pain 'cp':\n",
    "# 0 = female related symptoms[former 2]),\n",
    "# 1 = male related symptoms [former 1 and 3],\n",
    "# 2 = asymptomatic [former 4].\n",
    "if 4 in list(df3['cp'].unique()):\n",
    "    df3['cp'] = df3['cp'].replace(to_replace=[1, 2, 3, 4], value=[1, 0, 1, 2])\n",
    "else:\n",
    "    print('Already adjusted.')\n",
    "\n",
    "# 'thal' values only need a reordination.\n",
    "if 7 in list(df3['thal'].unique()):\n",
    "    df3['thal'] = df3['thal'].replace(to_replace=[3, 6, 7], value=[0, 1, 2])\n",
    "else:\n",
    "    print('Already adjusted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firts, let's define our X, y data.\n",
    "X = df4.drop(columns=['num'])\n",
    "y = df4['num']\n",
    "\n",
    "# Input (X) and target (y) selection\n",
    "print('Data shape - X: {}, y: {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my planning I was going to write my own frunction for\n",
    "# this, but why?\n",
    "# From the scikit-learn documentation, we can use the\n",
    "# train_test_split helper function to split our data and\n",
    "# save a replicable state.\n",
    "#\n",
    "# Replicable state number 11,\n",
    "# Test sample of 20%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=11)\n",
    "\n",
    "print('Training data sample - X: {}, y: {}'.format(X_train.shape, y_train.shape))\n",
    "print('Testing data sample - X: {}, y: {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our cross-validation will divide our training data\n",
    "# in 4 groups, to make it close to the size of our test\n",
    "# sample.\n",
    "#\n",
    "# First we run our baseline DecisionTreeClassifier.\n",
    "def dt_result_cv():\n",
    "    model = DecisionTreeClassifier()\n",
    "    dt_result_cv = recall_f1_cv(x_cv=X_train, y_cv=y_train, kfold=5, \n",
    "                                model_name='Decision Tree', model=model, verbose=False)\n",
    "\n",
    "    return dt_result_cv\n",
    "\n",
    "dt_result_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Random Forest\n",
    "def rf_result_cv():\n",
    "    model = RandomForestClassifier()\n",
    "    rf_result_cv = recall_f1_cv(x_cv=X_train, y_cv=y_train, kfold=5, \n",
    "                                model_name='Random Forest', model=model, verbose=False)\n",
    "\n",
    "    return rf_result_cv\n",
    "\n",
    "rf_result_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline XGBoost\n",
    "model = XGBClassifier()\n",
    "xgb_result_cv = recall_f1_cv(x_cv=X_train, y_cv=y_train, kfold=8, \n",
    "                             model_name='XGBoost', model=model, verbose=False)\n",
    "\n",
    "xgb_result_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "    random_state=None,\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=900,\n",
    "    eta=0.01,\n",
    "    max_depth=100,\n",
    "    min_samples_leaf=5,\n",
    "    min_child_weight=0,\n",
    "    colsample_bytree= 0.5,\n",
    "    subsample=0.5\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Choosing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dt_result_cv()\n",
    "result.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost is giving the same result everytime I run it.\n",
    "# So it's not gonna be used.\n",
    "#\n",
    "# Models that we can hypertune: DecisionTree and RandomForest.\n",
    "'''\n",
    "possible_models = [dt_result_cv(), rf_result_cv()]\n",
    "\n",
    "# Let's see how they perform on average for baseline.\n",
    "for m in possible_models:\n",
    "\n",
    "    model_result = pd.DataFrame(columns=['Model Name', 'Recall Score', 'F1 Score'])\n",
    "\n",
    "    for i in range(0, 50000):\n",
    "        model_result = pd.concat([model_result, m])\n",
    "\n",
    "    avg_recall = model_result['Recall Score'].astype(float).mean()\n",
    "    avg_recall = round(avg_recall, 2)\n",
    "    avg_f1 = model_result['F1 Score'].astype(float).mean()\n",
    "    avg_f1 = round(avg_f1, 2)\n",
    "\n",
    "    m_name = m.loc[0, 'Model Name']\n",
    "    print('Model Name: {}'.format(m_name))\n",
    "    print('Average Recall: {}\\nAverage F1: {}\\n'.format(avg_recall, avg_f1))\n",
    "'''\n",
    "# Running the comparison, we see that both have similar performance\n",
    "# on Recall, but Random Forest wins on F1.\n",
    "# Then we go with Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 HYPERPARAMETER FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on, we are gonna use Random Forest for fine tuning.\n",
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Grid Searching best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model.\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Let's define the parameters for our grid search.\n",
    "param_grid = {\n",
    "    'n_estimators': range(10, 51),\n",
    "    'max_depth' : range(2, 11),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_impurity_decrease': [0.1, 0.01]\n",
    "}\n",
    "\n",
    "# Setting GridSearchCV up.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print('Best parameters:')\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                                  max_depth=best_params['max_depth'],\n",
    "                                  max_features=best_params['max_features'],\n",
    "                                  random_state=11)\n",
    "\n",
    "# Train and predict.\n",
    "rf_tuned.fit(X_train, y_train)\n",
    "y_pred = rf_tuned.predict(X_test)\n",
    "\n",
    "# Show results on Recall and F1 scores.\n",
    "recall = round(recall_score(y_test, y_pred), 2)\n",
    "f1 = round(f1_score(y_test, y_pred), 2)\n",
    "print('Recall Score: {}'.format(recall))\n",
    "print('F1 Score: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Saving our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( rf_tuned, open('model/heart_disease_tuned_rf.pkl', 'wb') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 MODEL EVALUATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart_disease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
